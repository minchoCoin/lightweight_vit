{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6dd3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved linear.onnx\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import helper, TensorProto\n",
    "\n",
    "# 1️⃣ 입력과 출력 정의\n",
    "X = helper.make_tensor_value_info('X', TensorProto.FLOAT, [None, 3])\n",
    "Y = helper.make_tensor_value_info('Y', TensorProto.FLOAT, [None, 2])\n",
    "\n",
    "# 2️⃣ 가중치(W, b) 정의\n",
    "W = helper.make_tensor(\n",
    "    name='W',\n",
    "    data_type=TensorProto.FLOAT,\n",
    "    dims=[3, 2],\n",
    "    vals=[0.1, 0.2, 0.3,\n",
    "          0.4, 0.5, 0.6],  # row-major\n",
    ")\n",
    "\n",
    "b = helper.make_tensor(\n",
    "    name='b',\n",
    "    data_type=TensorProto.FLOAT,\n",
    "    dims=[2],\n",
    "    vals=[0.1, 0.2],\n",
    ")\n",
    "\n",
    "# 3️⃣ 노드(연산 그래프) 정의\n",
    "node1 = helper.make_node(\n",
    "    'MatMul',\n",
    "    inputs=['X', 'W'],\n",
    "    outputs=['WX']\n",
    ")\n",
    "\n",
    "node2 = helper.make_node(\n",
    "    'Add',\n",
    "    inputs=['WX', 'b'],\n",
    "    outputs=['Y']\n",
    ")\n",
    "\n",
    "# 4️⃣ 그래프 구성\n",
    "graph = helper.make_graph(\n",
    "    nodes=[node1, node2],\n",
    "    name='LinearModel',\n",
    "    inputs=[X],\n",
    "    outputs=[Y],\n",
    "    initializer=[W, b],\n",
    ")\n",
    "\n",
    "# 5️⃣ 모델 생성\n",
    "model = helper.make_model(graph, producer_name='custom-onnx-generator')\n",
    "onnx.save(model, 'linear.onnx')\n",
    "print(\"✅ Saved linear.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d85af3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved vit_tiny_opset13_bs1.onnx\n"
     ]
    }
   ],
   "source": [
    "# tiny_vit_opset13_bs1.py\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import helper, TensorProto\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameters (static)\n",
    "# ----------------------------\n",
    "IMG_H = IMG_W = 224\n",
    "PATCH = 16\n",
    "NUM_PATCH = (IMG_H // PATCH) * (IMG_W // PATCH)  # 14*14 = 196\n",
    "HIDDEN = 192\n",
    "NUM_HEADS = 3\n",
    "HEAD_DIM = HIDDEN // NUM_HEADS  # 64\n",
    "SEQ = NUM_PATCH + 1             # + class token = 197\n",
    "MLP_HID = 4 * HIDDEN            # 768\n",
    "NUM_CLASSES = 10                # demo\n",
    "BATCH = 1\n",
    "\n",
    "nodes, inits = [], []\n",
    "\n",
    "def add(op, inputs, outputs, **attrs):\n",
    "    nodes.append(helper.make_node(op, inputs=inputs, outputs=outputs, **attrs))\n",
    "\n",
    "def const_i64(name, vals):\n",
    "    t = helper.make_tensor(name, TensorProto.INT64, [len(vals)], vals)\n",
    "    inits.append(t); return name\n",
    "\n",
    "def const_shape(name, vals):  # reshape용 shape\n",
    "    t = helper.make_tensor(name, TensorProto.INT64, [len(vals)], vals)\n",
    "    inits.append(t); return name\n",
    "\n",
    "def const_f(name, arr):\n",
    "    arr = np.asarray(arr, dtype=np.float32)\n",
    "    t = helper.make_tensor(name, TensorProto.FLOAT, arr.shape, arr.ravel().tolist())\n",
    "    inits.append(t); return name\n",
    "\n",
    "# 자주 쓰는 axes/상수 텐서\n",
    "AX1_VEC = const_i64('AX1_VEC', [1])     # Squeeze axis=1\n",
    "AX2_VEC = const_i64('AX2_VEC', [2])     # Squeeze axis=2\n",
    "SPLIT_111 = const_i64('SPLIT_111', [1,1,1])\n",
    "SL_AX = const_i64('SL_AX', [0,1,2])\n",
    "SL_SP = const_i64('SL_SP', [1,1,1])\n",
    "\n",
    "# ----------------------------\n",
    "# IO (배치=1 고정)\n",
    "# ----------------------------\n",
    "X = helper.make_tensor_value_info('images', TensorProto.FLOAT, [BATCH, 3, IMG_H, IMG_W])\n",
    "Y = helper.make_tensor_value_info('logits', TensorProto.FLOAT, [BATCH, NUM_CLASSES])\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Patch Embedding: Conv(stride=16,kernel=16,out=HIDDEN) -> [1,H,14,14]\n",
    "# ----------------------------\n",
    "W_conv = (np.random.randn(HIDDEN, 3, PATCH, PATCH).astype(np.float32) * 0.02)\n",
    "B_conv = np.zeros((HIDDEN,), dtype=np.float32)\n",
    "inits += [\n",
    "    helper.make_tensor('W_patch', TensorProto.FLOAT, W_conv.shape, W_conv.ravel().tolist()),\n",
    "    helper.make_tensor('b_patch', TensorProto.FLOAT, B_conv.shape, B_conv.tolist()),\n",
    "]\n",
    "add('Conv', ['images', 'W_patch', 'b_patch'], ['patch'],\n",
    "    strides=[PATCH, PATCH], kernel_shape=[PATCH, PATCH])\n",
    "\n",
    "# [1,H,14,14] -> [1,H,196] -> [1,196,H]\n",
    "add('Reshape', ['patch', const_shape('SH_patch2', [BATCH, HIDDEN, NUM_PATCH])], ['patch_flat'])\n",
    "add('Transpose', ['patch_flat'], ['tokens'], perm=[0, 2, 1])\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Class token + Positional embedding (전부 고정 shape)\n",
    "# ----------------------------\n",
    "# cls [1,1,H], tokens [1,196,H] -> concat axis=1: [1,197,H]\n",
    "cls_token = np.zeros((1,1,HIDDEN), dtype=np.float32)\n",
    "const_f('CLS_TOKEN', cls_token)\n",
    "add('Concat', ['CLS_TOKEN', 'tokens'], ['tokens_with_cls'], axis=1)\n",
    "\n",
    "# pos [1,197,H] -> 더하기\n",
    "pos = (np.random.randn(1, SEQ, HIDDEN).astype(np.float32) * 0.02)\n",
    "const_f('POS_EMB', pos)\n",
    "add('Add', ['tokens_with_cls', 'POS_EMB'], ['x0'])\n",
    "\n",
    "# ----------------------------\n",
    "# 3) LayerNorm (eps=1e-5) on last dim\n",
    "# ----------------------------\n",
    "const_f('TWO', np.array([2.0], dtype=np.float32))\n",
    "const_f('EPS', np.array([1e-5], dtype=np.float32))\n",
    "\n",
    "add('ReduceMean', ['x0'], ['x0_mean'], axes=[-1], keepdims=1)\n",
    "add('Sub', ['x0', 'x0_mean'], ['x0_center'])\n",
    "add('Pow', ['x0_center', 'TWO'], ['x0_sq'])\n",
    "add('ReduceMean', ['x0_sq'], ['x0_var'], axes=[-1], keepdims=1)\n",
    "add('Add', ['x0_var', 'EPS'], ['x0_var_eps'])\n",
    "add('Sqrt', ['x0_var_eps'], ['x0_std'])\n",
    "add('Div', ['x0_center', 'x0_std'], ['x0_norm'])\n",
    "const_f('GAMMA1', np.ones((HIDDEN,), np.float32))\n",
    "const_f('BETA1',  np.zeros((HIDDEN,), np.float32))\n",
    "add('Mul', ['x0_norm', 'GAMMA1'], ['x0_ng'])\n",
    "add('Add', ['x0_ng', 'BETA1'], ['x1'])\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Multi-Head Self-Attention\n",
    "# ----------------------------\n",
    "W_qkv = (np.random.randn(HIDDEN, 3*HIDDEN).astype(np.float32) * 0.02)\n",
    "b_qkv = np.zeros((3*HIDDEN,), dtype=np.float32)\n",
    "const_f('W_QKV', W_qkv); const_f('B_QKV', b_qkv)\n",
    "add('MatMul', ['x1', 'W_QKV'], ['qkv_mm'])\n",
    "add('Add', ['qkv_mm', 'B_QKV'], ['qkv'])\n",
    "\n",
    "# reshape -> [1,SEQ,3,NUM_HEADS,HEAD_DIM]\n",
    "add('Reshape', ['qkv', const_shape('SH_QKV5', [BATCH, SEQ, 3, NUM_HEADS, HEAD_DIM])], ['qkv_5d'])\n",
    "\n",
    "# Split-13: split sizes 입력 텐서\n",
    "add('Split', ['qkv_5d', 'SPLIT_111'], ['Q_u', 'K_u', 'V_u'], axis=2)\n",
    "\n",
    "# Squeeze-13: axes는 두 번째 입력\n",
    "add('Squeeze', ['Q_u', 'AX2_VEC'], ['Q'])  # -> [1,SEQ,NUM_HEADS,HEAD_DIM]\n",
    "add('Squeeze', ['K_u', 'AX2_VEC'], ['K'])\n",
    "add('Squeeze', ['V_u', 'AX2_VEC'], ['V'])\n",
    "\n",
    "# -> [1,NUM_HEADS,SEQ,HEAD_DIM]\n",
    "add('Transpose', ['Q'], ['Qt'], perm=[0,2,1,3])\n",
    "add('Transpose', ['K'], ['Kt'], perm=[0,2,1,3])\n",
    "add('Transpose', ['V'], ['Vt'], perm=[0,2,1,3])\n",
    "# K^T\n",
    "add('Transpose', ['Kt'], ['KtT'], perm=[0,1,3,2])\n",
    "\n",
    "# scores = Q @ K^T / sqrt(HEAD_DIM)\n",
    "add('MatMul', ['Qt', 'KtT'], ['scores'])\n",
    "const_f('SCALE', np.array([1.0/np.sqrt(HEAD_DIM)], dtype=np.float32))\n",
    "add('Mul', ['scores', 'SCALE'], ['scores_s'])\n",
    "add('Softmax', ['scores_s'], ['attn'], axis=-1)\n",
    "\n",
    "# context\n",
    "add('MatMul', ['attn', 'Vt'], ['ctx'])               # [1,Hd,SEQ,HDIM]\n",
    "add('Transpose', ['ctx'], ['ctx_tr'], perm=[0,2,1,3])# [1,SEQ,Hd,HDIM]\n",
    "add('Reshape', ['ctx_tr', const_shape('SH_CTX2D', [BATCH, SEQ, HIDDEN])], ['ctx_2d'])\n",
    "\n",
    "# output proj + residual\n",
    "W_o = (np.random.randn(HIDDEN, HIDDEN).astype(np.float32) * 0.02)\n",
    "b_o = np.zeros((HIDDEN,), dtype=np.float32)\n",
    "const_f('W_O', W_o); const_f('B_O', b_o)\n",
    "add('MatMul', ['ctx_2d', 'W_O'], ['attn_out'])\n",
    "add('Add', ['attn_out', 'B_O'], ['attn_lin'])\n",
    "add('Add', ['attn_lin', 'x0'], ['res1'])\n",
    "\n",
    "# ----------------------------\n",
    "# 5) LayerNorm2 + MLP(ReLU) + Residual\n",
    "# ----------------------------\n",
    "add('ReduceMean', ['res1'], ['r1_mean'], axes=[-1], keepdims=1)\n",
    "add('Sub', ['res1', 'r1_mean'], ['r1_c'])\n",
    "add('Pow', ['r1_c', 'TWO'], ['r1_sq'])\n",
    "add('ReduceMean', ['r1_sq'], ['r1_var'], axes=[-1], keepdims=1)\n",
    "add('Add', ['r1_var', 'EPS'], ['r1_var_eps'])\n",
    "add('Sqrt', ['r1_var_eps'], ['r1_std'])\n",
    "add('Div', ['r1_c', 'r1_std'], ['r1_n'])\n",
    "const_f('GAMMA2', np.ones((HIDDEN,), np.float32))\n",
    "const_f('BETA2',  np.zeros((HIDDEN,), np.float32))\n",
    "add('Mul', ['r1_n', 'GAMMA2'], ['r1_ng'])\n",
    "add('Add', ['r1_ng', 'BETA2'], ['x2'])\n",
    "\n",
    "W1 = (np.random.randn(HIDDEN, MLP_HID).astype(np.float32) * 0.02)\n",
    "b1 = np.zeros((MLP_HID,), dtype=np.float32)\n",
    "W2 = (np.random.randn(MLP_HID, HIDDEN).astype(np.float32) * 0.02)\n",
    "b2 = np.zeros((HIDDEN,), dtype=np.float32)\n",
    "const_f('W1', W1); const_f('B1', b1); const_f('W2', W2); const_f('B2', b2)\n",
    "\n",
    "add('MatMul', ['x2', 'W1'], ['mlp1'])\n",
    "add('Add', ['mlp1', 'B1'], ['mlp1b'])\n",
    "add('Relu', ['mlp1b'], ['mlp_act'])\n",
    "add('MatMul', ['mlp_act', 'W2'], ['mlp2'])\n",
    "add('Add', ['mlp2', 'B2'], ['mlp_out'])\n",
    "add('Add', ['mlp_out', 'res1'], ['enc_out'])\n",
    "\n",
    "# ----------------------------\n",
    "# 6) CLS 추출 (Slice 고정 범위) → [1,H]\n",
    "# ----------------------------\n",
    "SL_ST = const_i64('SL_ST', [0, 0, 0])          # [B,SEQ,H]\n",
    "SL_ED = const_i64('SL_ED', [1, 1, HIDDEN])     # 배치=1, seq 0..1, hidden 0..H\n",
    "add('Slice', ['enc_out', 'SL_ST', 'SL_ED', 'SL_AX', 'SL_SP'], ['cls_1'])\n",
    "add('Squeeze', ['cls_1', 'AX1_VEC'], ['cls'])  # axis=1 제거 → [1,H]\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Classifier\n",
    "# ----------------------------\n",
    "Wc = (np.random.randn(HIDDEN, NUM_CLASSES).astype(np.float32) * 0.02)\n",
    "bc = np.zeros((NUM_CLASSES,), dtype=np.float32)\n",
    "const_f('Wc', Wc); const_f('bc', bc)\n",
    "add('MatMul', ['cls', 'Wc'], ['logits_mm'])\n",
    "add('Add', ['logits_mm', 'bc'], ['logits'])\n",
    "\n",
    "# ----------------------------\n",
    "# Build & Save\n",
    "# ----------------------------\n",
    "graph = helper.make_graph(nodes, 'TinyViT_OP13_BS1', [X], [Y], initializer=inits)\n",
    "model = helper.make_model(graph, producer_name='vit-onnx13-bs1',\n",
    "                          opset_imports=[helper.make_operatorsetid('', 13)])\n",
    "onnx.checker.check_model(model)\n",
    "onnx.save(model, 'vit_tiny_opset13_bs1.onnx')\n",
    "print('✅ Saved vit_tiny_opset13_bs1.onnx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pureonnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
