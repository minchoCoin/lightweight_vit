{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-YO4wQvMZer"
      },
      "outputs": [],
      "source": [
        "# pytorch_vit_to_onnx_int8.py\n",
        "import os, glob, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "DATA_ROOT = kagglehub.dataset_download(\"imsparsh/flowers-dataset\")\n",
        "print(DATA_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FqzwvH-QBuP",
        "outputId": "04317fbe-4c79-426c-dab0-24b6ca242bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'flowers-dataset' dataset.\n",
            "/kaggle/input/flowers-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
        "TEST_DIR  = os.path.join(DATA_ROOT, \"test\")\n",
        "\n",
        "IMG = 224\n",
        "BATCH = 1\n",
        "EPOCHS = 2\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "\n",
        "# ====== 1) RMSNorm (PyTorch) ======\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.use_bias = use_bias\n",
        "        self.gamma = nn.Parameter(torch.ones(dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(dim)) if use_bias else None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (..., dim)\n",
        "        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
        "        y = x / rms * self.gamma\n",
        "        if self.beta is not None:\n",
        "            y = y + self.beta\n",
        "        return y\n",
        "\n",
        "\n",
        "# ====== 2) Transformer Encoder 블록 ======\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim: int, heads: int, mlp_dim: int, dropout: float = 0.1, use_layernorm: bool = False):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-6) if use_layernorm else RMSNorm(dim, eps=1e-6)\n",
        "        self.attn  = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout, batch_first=True)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-6) if use_layernorm else RMSNorm(dim, eps=1e-6)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, N, dim]\n",
        "        h = self.norm1(x)\n",
        "        h, _ = self.attn(h, h, h, need_weights=False)  # self-attention\n",
        "        x = x + self.drop1(h)\n",
        "\n",
        "        h = self.norm2(x)\n",
        "        h = self.mlp(h)\n",
        "        return x + h\n",
        "\n",
        "\n",
        "# ====== 3) ViT (간단 버전, CLS 없이 GAP) ======\n",
        "class TinyViT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: int = 224,\n",
        "        patch: int = 16,\n",
        "        num_classes: int = 5,\n",
        "        dim: int = 128,\n",
        "        depth: int = 5,\n",
        "        heads: int = 8,\n",
        "        mlp_dim: int = 256,\n",
        "        dropout: float = 0.1,\n",
        "        use_layernorm: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert image_size % patch == 0\n",
        "        num_patches = (image_size // patch) ** 2\n",
        "        self.patch = patch\n",
        "        self.dim = dim\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        # Patch embedding: Conv + reshape\n",
        "        self.patch_embed = nn.Conv2d(3, dim, kernel_size=patch, stride=patch, padding=0)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        self.encoders = nn.ModuleList([\n",
        "            TransformerEncoder(dim=dim, heads=heads, mlp_dim=mlp_dim, dropout=dropout, use_layernorm=use_layernorm)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.final_norm = nn.LayerNorm(dim, eps=1e-6) if use_layernorm else RMSNorm(dim, eps=1e-6)\n",
        "        self.head = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        '''\n",
        "        # x: [B, 3, H, W], H=W=image_size\n",
        "        x = self.patch_embed(x)                 # [B, dim, H/ps, W/ps]\n",
        "        x = x.flatten(2).transpose(1, 2)        # [B, N, dim]\n",
        "        x = x + self.pos_embed                  # [B, N, dim]\n",
        "        for blk in self.encoders:\n",
        "            x = blk(x)\n",
        "        x = self.final_norm(x)\n",
        "        x = x.mean(dim=1)                       # GAP over tokens\n",
        "        logits = self.head(x)                   # [B, num_classes]\n",
        "        return logits\n",
        "        '''\n",
        "        # x: [1, 3, 224, 224] (배치 1로 고정)\n",
        "        x = self.patch_embed(x)                           # [1, dim, Hp, Wp]\n",
        "        x = x.permute(0, 2, 3, 1).contiguous()            # [1, Hp, Wp, dim]\n",
        "        x = x.view(1, self.num_patches, self.dim)         # <-- 완전 상수 shape\n",
        "        x = x + self.pos_embed                            # [1, N, dim]\n",
        "        for blk in self.encoders:\n",
        "          x = blk(x)\n",
        "        x = self.final_norm(x)\n",
        "        x = x.mean(dim=1)                                 # [1, dim]\n",
        "        logits = self.head(x)                             # [1, num_classes]\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ====== 4) 데이터셋/전처리 ======\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.Resize((IMG, IMG)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),                    # [0,1]\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "def build_loaders(batch=BATCH):\n",
        "    train_ds = datasets.ImageFolder(root=TRAIN_DIR, transform=train_tf)\n",
        "    #val_ds   = datasets.ImageFolder(root=TEST_DIR,  transform=val_tf)\n",
        "    train_ld = DataLoader(train_ds, batch_size=batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    #val_ld   = DataLoader(val_ds,   batch_size=batch, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    return train_ld\n",
        "\n",
        "\n",
        "# ====== 5) 간단 학습 루프 ======\n",
        "def train_one_epoch(model, loader, optim, device):\n",
        "    model.train()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits, y)\n",
        "        loss_sum += loss.item() * x.size(0)\n",
        "        pred = logits.argmax(dim=1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += x.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = TinyViT(\n",
        "        image_size=IMG, patch=16, num_classes=NUM_CLASSES,\n",
        "        dim=128, depth=5, heads=8, mlp_dim=256, dropout=0.1,\n",
        "        use_layernorm=False\n",
        "    ).to(device)\n",
        "\n",
        "train_ld = build_loaders()\n",
        "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_ld, optim, device)\n",
        "        print(f\"[{epoch+1}/{EPOCHS}] train loss {tr_loss:.4f} acc {tr_acc:.3f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"tinyvit.pt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sgrqp76sQE35",
        "outputId": "8d508f84-0871-44ea-80f1-4bba3a2d4110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/2] train loss 1.4591 acc 0.329\n",
            "[2/2] train loss 1.2114 acc 0.473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime onnxsim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTB-yZLdT5nL",
        "outputId": "4be0a85c-8ec1-4fe4-b3fe-87869f0cd7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36.tar.gz (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.3)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from onnxsim) (13.9.4)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: onnxsim\n",
            "  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnxsim: filename=onnxsim-0.4.36-cp312-cp312-linux_x86_64.whl size=2200379 sha256=2dd3d3ff768afd05f90bee8fb8297e61e86b4316d1ea9ff3f453328b2ca0af6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/5d/cc/db1350d9fabfe7f8442b5d97aff2ff543fc253277f71a6508f\n",
            "Successfully built onnxsim\n",
            "Installing collected packages: humanfriendly, onnx, coloredlogs, onnxsim, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.19.1 onnxruntime-1.23.2 onnxsim-0.4.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 더미 입력: 고정 배치1\n",
        "dummy = torch.randn(1, 3, IMG, IMG, dtype=torch.float32)\n",
        "model.to('cpu')\n",
        "# 안전한 opset 13 (Neural ART/임베디드 호환 용이)\n",
        "torch.onnx.export(\n",
        "    model, (dummy,), \"tinyvit_fp32.onnx\",\n",
        "    input_names=[\"input\"], output_names=[\"logits\"],\n",
        "    opset_version=14,\n",
        "    do_constant_folding=False,\n",
        "    dynamic_axes=None  # 고정 입력(1x3x224x224). 필요하면 {\"input\":{0:\"B\"}}로 변경\n",
        ")\n",
        "print(\"Exported: tinyvit_fp32.onnx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9b8dmoxRgJM",
        "outputId": "c859a00e-9742-4b75-d201-b285ca743bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1985123791.py:5: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported: tinyvit_fp32.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxsim import simplify\n",
        "\n",
        "# load your predefined ONNX model\n",
        "model = onnx.load('tinyvit_fp32.onnx')\n",
        "\n",
        "# convert model\n",
        "model_simp, check = simplify(model)\n",
        "\n",
        "assert check, \"Simplified ONNX model could not be validated\"\n",
        "onnx.save(model_simp,'tinyvit_fp32_sim.onnx')\n"
      ],
      "metadata": {
        "id": "K-u333TPxOzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m onnxruntime.quantization.preprocess --input tinyvit_fp32_sim.onnx --output tinyvit_fp32_infer.onnx"
      ],
      "metadata": {
        "id": "86w_2vCKUQ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantize_static_qdq.py\n",
        "import os, glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, CalibrationMethod, QuantType\n",
        "from onnxruntime.quantization import preprocess\n",
        "from onnxruntime import InferenceSession\n",
        "\n",
        "IMG = 224\n",
        "TEST_DIR = os.getenv(\"FLOWERS_TEST_DIR\", \"/kaggle/input/flowers-dataset/test\")\n",
        "\n",
        "INPUT_NAME = \"input\"   # export_onnx.py에서 지정\n",
        "ONNX_IN  = \"tinyvit_fp32_sim.onnx\"\n",
        "ONNX_INF = \"tinyvit_fp32_infer.onnx\"\n",
        "ONNX_INT8 = \"tinyvit_int8_qdq.onnx\"\n",
        "\n",
        "\n",
        "\n",
        "# 3-2) 캘리브레이터\n",
        "class ImageFolderDataReader(CalibrationDataReader):\n",
        "    def __init__(self, folder, input_name, img_size=224, max_images=200):\n",
        "        self.input_name = input_name\n",
        "        self.img_paths = sorted(\n",
        "            sum([glob.glob(os.path.join(folder, ext)) for ext in (\"*.jpg\", \"*.png\", \"*.jpeg\")], [])\n",
        "        )\n",
        "        if not self.img_paths:\n",
        "            raise FileNotFoundError(f\"No images found under {folder}\")\n",
        "        self.img_paths = self.img_paths[:max_images]\n",
        "        self.enum_data = None\n",
        "        self.count = 0\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.enum_data is None:\n",
        "            self.enum_data = self._data_iter()\n",
        "        return next(self.enum_data, None)\n",
        "\n",
        "    def _data_iter(self):\n",
        "        for p in self.img_paths:\n",
        "            img = Image.open(p).convert(\"RGB\").resize((IMG, IMG), Image.BILINEAR)\n",
        "            arr = np.asarray(img, dtype=np.float32) / 255.0      # [H,W,3] in [0,1]\n",
        "            arr = np.transpose(arr, (2,0,1))                     # [3,H,W]\n",
        "            arr = np.expand_dims(arr, 0)                         # [1,3,H,W]\n",
        "            yield { self.input_name: arr }\n",
        "\n",
        "# 3-3) 정적 Q/DQ 양자화\n",
        "dr = ImageFolderDataReader(TEST_DIR, INPUT_NAME, img_size=IMG, max_images=200)\n",
        "quantize_static(\n",
        "    model_input=ONNX_INF,\n",
        "    model_output=ONNX_INT8,\n",
        "    calibration_data_reader=dr,\n",
        "    #calibration_method=CalibrationMethod.MinMax,   # 필요시 Percentile/Entropy로 변경\n",
        "    per_channel=True,                              # conv/linear에 유리\n",
        "    reduce_range=False,\n",
        "    weight_type=QuantType.QInt8,                   # 가중치 INT8\n",
        "    activation_type=QuantType.QInt8                # 활성값 INT8 (임베디드 친화)\n",
        ")\n",
        "print(\"Quantized:\", ONNX_INT8)\n",
        "\n",
        "# 3-4) 간단 검증\n",
        "sess = InferenceSession(ONNX_INT8, providers=[\"CPUExecutionProvider\"])\n",
        "print(\"Inputs:\", [i.name for i in sess.get_inputs()], sess.get_inputs()[0].shape, sess.get_inputs()[0].type)\n",
        "print(\"Outputs:\", [o.name for o in sess.get_outputs()], sess.get_outputs()[0].shape, sess.get_outputs()[0].type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b541WyqARkQ6",
        "outputId": "323d2552-a39f-4ac0-a994-79b26fb76cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized: tinyvit_int8_qdq.onnx\n",
            "Inputs: ['input'] [1, 3, 224, 224] tensor(float)\n",
            "Outputs: ['logits'] [1, 5] tensor(float)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxsim import simplify\n",
        "\n",
        "# load your predefined ONNX model\n",
        "model = onnx.load('tinyvit_int8_qdq.onnx')\n",
        "\n",
        "# convert model\n",
        "model_simp, check = simplify(model)\n",
        "\n",
        "assert check, \"Simplified ONNX model could not be validated\"\n",
        "onnx.save(model_simp,'tinyvit_int8_sim.onnx')\n"
      ],
      "metadata": {
        "id": "Ma_JtiFG-hzK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}